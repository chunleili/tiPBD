import taichi as ti
import numpy as np
import scipy
import scipy.sparse as sp
import os, sys
from time import perf_counter
from matplotlib import pyplot as plt
import matplotlib
import pyamg
from pyamg.gallery import poisson
from pyamg.multilevel import MultilevelSolver
from pyamg.strength import classical_strength_of_connection
from pyamg.classical.interpolate import direct_interpolation
from pyamg.classical.split import RS

sys.path.append(os.getcwd())

# global r_norm_list
# r_norm_list = []


def test_amg_vs_jacobi():
    # # # ------------------------------- data prepare ------------------------------- #
    # print("loading data...")
    # A = scipy.io.mmread("A.mtx")
    # A = A.tocsr()
    # b = np.loadtxt("b.txt")
    # R = scipy.io.mmread("R.mtx")
    # # P = scipy.io.mmread("P.mtx")
    # # R = sp.random(1000, 34295, density=0.001, dtype=bool)
    # P = R.transpose()
    # print(f"R: {R.shape}, P: {P.shape}, A: {A.shape}, b1: {b.shape}")

    # # # ------------------------------- data prepare ------------------------------- #
    # print("loading data...")
    # A = scipy.io.mmread("A.mtx")
    # A = A.tocsr()
    # b = np.loadtxt("b.txt")
    # n = b.shape[0]
    # R1 = sp.random(int(n/10), n, density=0.01, dtype=bool)
    # P1 = R1.transpose()
    # R2 = sp.random(int(n/100), int(n/10), density=0.01, dtype=bool)
    # P2 = R2.transpose()
    # R3 = sp.random(int(n/1000), int(n/100), density=0.01, dtype=bool)
    # P3 = R3.transpose()
    # R = [R1, R2, R3]
    # P = [P1, P2, P3]
    # print(f"A: {A.shape}")

    # # --------------------------- generate a SPD matrix -------------------------- #
    # # # prepare data
    # n = 3000  # Size of the matrix
    # random_matrix = np.random.rand(n, n)
    # symmetric_matrix = random_matrix @ random_matrix.T  # Generate a symmetric matrix
    # positive_definite_matrix = (
    #     symmetric_matrix + np.eye(n) * 1
    # ) * 10  # Add a small diagonal to make it positive definite
    # print("Random Symmetric Positive Definite Matrix:")
    # print(positive_definite_matrix)
    # A = positive_definite_matrix
    # b = np.random.rand(n)
    # A = sp.csr_matrix(A)

    # R1 = sp.random(int(n/10), n, density=0.01, dtype=bool)
    # P1 = R1.transpose()
    # R2 = sp.random(int(n/100), int(n/10), density=0.01, dtype=bool)
    # P2 = R2.transpose()
    # R3 = sp.random(int(n/1000), int(n/100), density=0.01, dtype=bool)
    # P3 = R3.transpose()
    # R = [R1, R2, R3]
    # P = [P1, P2, P3]
    # print(f"R: {R1.shape}, P: {P1.shape}, A: {A.shape}, b: {b.shape}")

    # ---------------------- data generated by pyamg poisson --------------------- #
    # n = 200
    # X, Y = np.meshgrid(np.linspace(0, 1, n), np.linspace(0, 1, n))
    # stencil = pyamg.gallery.diffusion_stencil_2d(type='FE', epsilon=0.001, theta=np.pi / 3)
    # A = pyamg.gallery.stencil_grid(stencil, (n, n), format='csr')
    # b = np.random.rand(A.shape[0])                     # pick a random right hand side

    A = poisson((10, 10), format="csr")
    b = np.random.rand(A.shape[0])
    print(f"A: {A.shape}, b: {b.shape}")

    # generate R by pyamg
    ml = pyamg.classical.ruge_stuben_solver(A, max_levels=2)  # construct the multigrid hierarchy
    print("ml bulit")
    P = ml.levels[0].P
    R = ml.levels[0].R

    # ------------------------------- test solvers ------------------------------- #

    use_AMG = True
    use_direct = True
    use_jacobi = True
    use_gs = True
    use_sor = True
    use_pyamg = True

    # AMG
    if use_AMG:
        r_norm_list_amg = []
        t = perf_counter()
        x0 = np.zeros_like(b)
        x_amg, r_amg = solve_amg(A, b, x0, R, P, r_norm_list_amg)
        t_amg = perf_counter() - t
        t = perf_counter()

    # Direct solver
    if use_direct:
        t = perf_counter()
        x0 = np.zeros_like(b)
        x_direct = scipy.sparse.linalg.spsolve(A, b)
        t_direct = perf_counter() - t
        t = perf_counter()

    # Jacobi
    if use_jacobi:
        r_norm_list_jacobi = []
        t = perf_counter()
        x0 = np.zeros_like(b)
        x_jacobi, r_jacobi = solve_jacobi_sparse(A, b, x0, 50, 1e-5, r_norm_list_jacobi)
        t_jacobi = perf_counter() - t
        t = perf_counter()

    # Gauss-Seidel
    if use_gs:
        r_norm_list_gs = []
        t = perf_counter()
        x0 = np.zeros_like(b)
        x_gs, r_gs = solve_gauss_seidel_sparse(A, b, x0, 50, 1e-5, r_norm_list_gs)
        t_gs = perf_counter() - t
        t = perf_counter()

    # SOR
    if use_sor:
        r_norm_list_sor = []
        t = perf_counter()
        x0 = np.zeros_like(b)
        x_sor, r_sor = solve_sor(A, b, x0, 1.25, 50, 1e-5, r_norm_list_sor)
        t_sor = perf_counter() - t
        t = perf_counter()

    # pyamg
    if use_pyamg:
        r_norm_list_pyamg = []
        t = perf_counter()
        x0 = np.zeros_like(b)
        x_pyamg = solve_pyamg(A, b, x0, R, P, r_norm_list_pyamg)
        t_pyamg = perf_counter() - t
        t = perf_counter()

    # ------------------------------- print results ------------------------------- #
    if use_direct:
        print(f"Direct solver time: {t_direct:.2e}")
    if use_AMG:
        print(f"AMG time: {t_amg:.2e}")
    if use_jacobi:
        print(f"jacobi time: {t_jacobi:.2e}")
    if use_gs:
        print(f"Gauss-Seidel time: {t_gs:.2e}")
    if use_sor:
        print(f"SOR time: {t_sor:.2e}")

    # ------------------------------- plot ------------------------------- #
    fig, axs = plt.subplots(2, 3, figsize=(10, 4))

    if use_AMG:
        plot_r_norm_list(r_norm_list_amg, axs[0, 0], "AMG")
    if use_jacobi:
        plot_r_norm_list(r_norm_list_jacobi, axs[0, 1], "Jacobi")
    if use_gs:
        plot_r_norm_list(r_norm_list_gs, axs[1, 0], "Gauss-Seidel")
    if use_sor:
        plot_r_norm_list(r_norm_list_sor, axs[1, 1], "SOR Omega=1.25")
    if use_pyamg:
        plot_r_norm_list(r_norm_list_pyamg, axs[0, 2], "pyamg")

    plt.tight_layout()
    plt.show()

    if use_AMG:
        print(f"AMG: max diff with direct solver: {np.linalg.norm(x_amg-x_direct, np.inf)}")
        assert np.allclose(x_amg, x_direct, atol=1e-4), f"max diff: {np.linalg.norm(x_amg-x_direct, np.inf)}"
    if use_jacobi:
        print(f"Jacobi: max diff with direct solver: {np.linalg.norm(x_jacobi-x_direct, np.inf)}")
        assert np.allclose(x_jacobi, x_direct, atol=1e-4), f"max diff: {np.linalg.norm(x_jacobi-x_direct, np.inf)}"
    if use_gs:
        print(f"Gauss-Seidel: max diff with direct solver: {np.linalg.norm(x_gs-x_direct, np.inf)}")
        assert np.allclose(x_gs, x_direct, atol=1e-4), f"max diff: {np.linalg.norm(x_gs-x_direct, np.inf)}"
    if use_sor:
        print(f"SOR: max diff with direct solver: {np.linalg.norm(x_sor-x_direct, np.inf)}")
        assert np.allclose(x_sor, x_direct, atol=1e-4), f"max diff: {np.linalg.norm(x_sor-x_direct, np.inf)}"
    print("All solutions is correct!\n")


def jacobi_iteration(A, b, x0, max_iterations=100, tolerance=1e-6, r_norm_list=[]):
    n = len(b)
    x = x0.copy()  # 初始解向量
    x_new = np.zeros_like(x)  # 存储更新后的解向量
    for iteration in range(max_iterations):
        for i in range(n):
            sum_term = np.dot(A[i, :n], x[:n]) - A[i, i] * x[i]
            x_new[i] = (b[i] - sum_term) / A[i, i]

        r = b - np.dot(A, x_new)
        r_norm = np.linalg.norm(r)
        r_norm_list.append(r_norm)
        print(f"Jacobi iter: {iteration}, res: {r_norm:.2e}")
        if r_norm < tolerance:
            break

        x = x_new.copy()
    return x_new, r


def solve_amg(A1, b, x0, R, P, r_norm_list=[]):
    print("\n----------start AMG-----------")

    # x1 initial guess
    x1 = x0
    r1 = b - A1 @ x1
    print(f"r1 initial:{np.linalg.norm(r1)}")
    r_norm_list.append(np.linalg.norm(r1))

    # 1. pre-smooth jacobi
    print(">>> 1. pre-smooth")
    print(f"r1 before pre-smooth:{np.linalg.norm(r1):.2e}")
    x1, r1 = solve_gauss_seidel_sparse(A1, b, x1, max_iterations=3, tolerance=1e-2)
    print(f"r1 after pre-smooth:{np.linalg.norm(r1):.2e}")
    r_norm_list.append(np.linalg.norm(r1))

    # 2 restriction: pass r1 to r2 and construct A2
    print(">>> 2. restriction")
    # print(R.shape, r1.shape, P.shape, A1.shape)
    r2 = R @ r1
    A2 = R @ A1 @ P

    # 3 solve coarse level A2E2=r2
    print(">>> 3. solve coarse")
    E2 = scipy.sparse.linalg.spsolve(A2, r2)
    # E2 = np.linalg.solve(A2, r2)

    # 4 prolongation: get E1 and add to x1
    print(">>> 4. prolongate")
    E1 = P @ E2
    x1 += E1

    print(f"r1 before solve coarse:{ np.linalg.norm(r1):.2e}")
    r1 = b - A1 @ x1
    print(f"r1 after solve coarse:{ np.linalg.norm(r1):.2e}")
    r_norm_list.append(np.linalg.norm(r1))

    # 5 post-smooth jacobi
    print(">>> 5. post-smooth")
    print(f"r1 before post-smooth:{np.linalg.norm(r1):.2e}")
    x1, r1 = solve_gauss_seidel_sparse(A1, b, x1, max_iterations=3, tolerance=1e-5)
    print(f"r1 after post-smooth:{np.linalg.norm(r1):.2e}")
    r_norm_list.append(np.linalg.norm(r1))

    x = x1
    print("----------finish AMG-----------\n")
    return x, r1


def solve_pyamg(A, b, x0, R, P, r_norm_list=[]):
    ml = pyamg.classical.ruge_stuben_solver(A, max_levels=2)  # construct the multigrid hierarchy
    ml.levels[0].R = R
    ml.levels[0].P = P
    x = ml.solve(b, tol=1e-3, residuals=r_norm_list, maxiter=1)  # solve Ax=b to a tolerance of 1e-12
    return x


def solve_amg_recursive(A, b, x0, R, P, level=0, r_norm_list=[]):
    print("\n----------start AMG-----------")
    print(f"level: {level}")

    # x1 initial guess
    x1 = x0
    r1 = b - A @ x1
    print(f"r1 initial:{np.linalg.norm(r1)}")
    r_norm_list.append(np.linalg.norm(r1))

    # 1. pre-smooth jacobi
    print(">>> 1. pre-smooth")
    print(f"r1 before pre-smooth:{np.linalg.norm(r1):.2e}")
    # x1, r1 = solve_gauss_seidel_sparse(A, b, x1, max_iterations=50, tolerance=1e-2)
    print(f"r1 after pre-smooth:{np.linalg.norm(r1):.2e}")
    r_norm_list.append(np.linalg.norm(r1))

    # 2 restriction: pass r1 to r2 and construct A2
    print(">>> 2. restriction")
    # print(R.shape, r1.shape, P.shape, A.shape)
    r2 = R[0] @ r1
    A2 = R[0] @ A @ P[0]

    # 3 solve coarse level A2E2=r2
    print(">>> 3. solve coarse")
    # E2 = scipy.sparse.linalg.spsolve(A2, r2)
    # E2 = np.linalg.solve(A2, r2)
    # E2, _ = solve_amg_recursive(A2, r2, np.zeros_like(r2), R, P, level+1)
    r3 = R[1] @ r2
    A2 = R[1] @ A2 @ P[1]

    # bottom level solve
    E3 = scipy.sparse.linalg.spsolve(A2, r3)

    # 4 prolongation: get E1 and add to x1
    print(">>> 4. prolongate")
    E2 = P[1] @ E3
    E1 = P[0] @ E2

    # correction to original x
    x1 += E1

    print(f"r1 before solve coarse:{ np.linalg.norm(r1):.2e}")
    r1 = b - A @ x1
    print(f"r1 after solve coarse:{ np.linalg.norm(r1):.2e}")
    r_norm_list.append(np.linalg.norm(r1))

    # 5 post-smooth jacobi
    print(">>> 5. post-smooth")
    print(f"r1 before post-smooth:{np.linalg.norm(r1):.2e}")
    # x1, r1 = solve_gauss_seidel_sparse(A, b, x1, max_iterations=100, tolerance=1e-5)
    print(f"r1 after post-smooth:{np.linalg.norm(r1):.2e}")
    r_norm_list.append(np.linalg.norm(r1))

    x = x1
    print("----------finish AMG-----------\n")
    return x, r1


def solve_amg_yp(A, f, x, R, P, r_norm_list=[]):
    # step 1: smoother
    x, _ = solve_gauss_seidel_sparse(A, np.zeros_like(f), f, max_iterations=5)

    # step 2: solve on coarsest level
    r = f - A @ x
    r_norm = np.linalg.norm(r)
    r_norm_list.append(r_norm)
    print(f"r_norm: {r_norm}")

    # A_coarse, R = setup(A)
    A_coarse = R @ A @ P

    # print(f"A size: {A.shape}")
    # print(f"R size: {R.shape}")
    # print(f"A_coarse size: {A_coarse.shape} ")
    g = R @ r

    x_coarse = sp.linalg.spsolve(A_coarse, np.transpose(g))
    # step 3: prolongate and correction
    d = P @ x_coarse
    x_new = np.zeros((f.shape[0],), dtype=np.float64)
    x_new = x + d

    r_new = f - A @ x_new
    r_new_norm = np.linalg.norm(r_new)
    r_norm_list.append(r_new_norm)
    print(f"r_new_norm: {r_new_norm}")

    # # step 4: post smoother
    x_new, _ = solve_gauss_seidel_sparse(A, np.zeros_like(f), f, max_iterations=5)
    # return x_post
    return x_new, r_new


# ---------------------------------------------------------------------------- #
#                                 Ax=b solvers                                 #
# ---------------------------------------------------------------------------- #


def solve_jacobi_ti(A, b, x0, max_iterations=100, tolerance=1e-6, r_norm_list=[]):
    print("Solving Ax=b using jacobi, taichi implementation...")
    n = A.shape[0]
    x = x0.copy()

    r = b - (A @ x)
    r_norm = np.linalg.norm(r)
    r_norm_list.append(r_norm)
    print(f"initial residual: {r_norm:.2e}")

    for iter in range(max_iterations):
        x_new = x.copy()
        jacobi_iter_once_kernel(A, b, x, x_new)
        x = x_new.copy()

        # 计算残差并检查收敛
        r = A @ x - b
        r_norm = np.linalg.norm(r)
        r_norm_list.append(r_norm)
        print(f"Jacobi iter {iter}, r={r_norm:.2e}")
        if r_norm < tolerance:
            print(f"Converged after {iter + 1} iterations. Final residual: {r_norm:.2e}")
            return x, r

    print("Did not converge within the maximum number of iterations.")
    print(f"Final residual: {r_norm:.2e}")
    return x, r


@ti.kernel
def jacobi_iter_once_kernel(
    A: ti.types.ndarray(), b: ti.types.ndarray(), x: ti.types.ndarray(), x_new: ti.types.ndarray()
):
    n = b.shape[0]
    for i in range(n):
        r = b[i]
        for j in range(n):
            if i != j:
                r -= A[i, j] * x[j]
        x_new[i] = r / A[i, i]


def solve_jacobi_sparse(A, b, x0, max_iterations=100, tolerance=1e-6, r_norm_list=[]):
    n = len(b)
    x = x0.copy()  # 初始解向量
    x_new = x0.copy()  # 存储更新后的解向量
    L = scipy.sparse.tril(A, k=-1)
    U = scipy.sparse.triu(A, k=1)
    D = A.diagonal()
    D_inv = 1.0 / D[:]
    D_inv = scipy.sparse.diags(D_inv)

    r = b - (A @ x_new)
    r_norm = np.linalg.norm(r)
    print(f"initial residual: {r_norm:.2e}")

    for iter in range(max_iterations):
        x_new = D_inv @ (b - (L + U) @ x)

        x = x_new.copy()

        # 计算残差并检查收敛
        r = A @ x - b
        r_norm = np.linalg.norm(r)
        r_norm_list.append(r_norm)
        print(f"Jacobi iter {iter}, r={r_norm:.2e}")

        if r_norm < tolerance:
            print(f"Converged after {iter + 1} iterations. Final residual: {r_norm:.2e}")
            return x, r

    print("Did not converge within the maximum number of iterations.")
    print(f"Final residual: {r_norm:.2e}")
    return x, r


# # 定义稀疏矩阵的雅可比迭代函数
# def jacobi_iteration_sparse(A, b, x0, max_iterations=100, tolerance=1e-6):
#     n = len(b)
#     x = x0.copy()  # 初始解向量
#     x_new = np.zeros_like(x)  # 存储更新后的解向量
#     L = scipy.sparse.tril(A, k=-1)
#     U = scipy.sparse.triu(A, k=1)
#     D = A.diagonal()
#     D_inv = 1.0 / D[:]
#     D_inv = scipy.sparse.diags(D_inv)
#     for iteration in range(max_iterations):
#         x_new = D_inv @ (b - (L + U) @ x)

#         residual = b - (A @ x_new)
#         r_norm = np.linalg.norm(residual)
#         if r_norm < tolerance:
#             break
#         print(f"jacobi iter: {iteration}, residual: {r_norm}")
#         x = x_new.copy()
#     return x_new, residual


def solve_sor_sparse_legacy(A, b, x0, omega=1.5, max_iterations=100, tolerance=1e-6, r_norm_list=[]):
    n = A.shape[0]
    x = x0.copy()
    for iter in range(max_iterations):
        new_x = np.copy(x)
        for i in range(A.shape[0]):
            start_idx = A.indptr[i]
            end_idx = A.indptr[i + 1]
            row_sum = A.data[start_idx:end_idx] @ new_x[A.indices[start_idx:end_idx]]
            x[i] = new_x[i] + omega * (b[i] - row_sum) / A.data[start_idx:end_idx].sum()

        # 计算残差并检查收敛
        r = A @ x - b
        r_norm = np.linalg.norm(r)
        r_norm_list.append(r_norm)
        print(f"SOR iter {iter}, r={r_norm:.2e}")
        if r_norm < tolerance:
            print(f"Converged after {iter + 1} iterations. Final residual: {r_norm:.2e}")
            return x, r

    print("Did not converge within the maximum number of iterations.")
    print(f"Final residual: {r_norm:.2e}")
    return x, r


def solve_sor_legacy(A, b, x0, omega=1.5, max_iterations=100, tolerance=1e-6, r_norm_list=[]):
    n = A.shape[0]
    x = x0.copy()

    # D = np.diag(A)
    # L = np.tril(A, k=-1)
    # U = np.triu(A, k=1)
    # Lw = np.linalg.inv(D + omega * L) @ (- omega * U + (1 - omega) * D )
    # spectral_radius_Lw = max(abs(np.linalg.eigvals(Lw)))
    # print(f"spectral radius of Lw: {spectral_radius_Lw:.2f}")

    for iter in range(max_iterations):
        x_new = x.copy()

        for i in range(n):
            x_new[i] = (1 - omega) * x[i] + (omega / A[i, i]) * (
                b[i] - np.dot(A[i, :i], x_new[:i]) - np.dot(A[i, i + 1 :], x[i + 1 :])
            )

        x = x_new.copy()

        # 计算残差并检查收敛
        r = A @ x - b
        r_norm = np.linalg.norm(r)
        r_norm_list.append(r_norm)
        print(f"SOR iter {iter}, r={r_norm:.2e}")
        if r_norm < tolerance:
            print(f"Converged after {iter + 1} iterations. Final residual: {r_norm:.2e}")
            return x, r

    print("Did not converge within the maximum number of iterations.")
    print(f"Final residual: {r_norm:.2e}")
    return x, r


def solve_sor(A, b, x0, omega=1.5, max_iterations=100, tolerance=1e-6, r_norm_list=[]):
    n = A.shape[0]
    x = x0.copy()

    for iter in range(max_iterations):
        x_new = x.copy()

        for i in range(n):
            Ax_new = A[i, :i].dot(x_new[:i])
            Ax_old = A[i, i + 1 :].dot(x[i + 1 :])
            x_new[i] = (1 - omega) * x[i] + (omega / A[i, i]) * (b[i] - Ax_new - Ax_old)

        x = x_new.copy()

        # 计算残差并检查收敛
        r = A @ x - b
        r_norm = np.linalg.norm(r)
        r_norm_list.append(r_norm)
        print(f"SOR iter {iter}, r={r_norm:.2e}")
        if r_norm < tolerance:
            print(f"Converged after {iter + 1} iterations. Final residual: {r_norm:.2e}")
            return x, r

    print("Did not converge within the maximum number of iterations.")
    print(f"Final residual: {r_norm:.2e}")
    return x, r


def solve_direct_solver(A, b):
    t = perf_counter()
    solver = ti.linalg.SparseSolver(solver_type="LLT")
    solver.analyze_pattern(A)
    solver.factorize(A)
    x = solver.solve(b)
    print(f"time: {perf_counter() - t}")
    print(f"shape of A: {A.shape}")
    print(f"solve success: {solver.info()}")
    return x


@ti.kernel
def gauss_seidel_kernel(A: ti.types.ndarray(), b: ti.types.ndarray(), x: ti.types.ndarray(), xOld: ti.types.ndarray()):
    N = b.shape[0]
    for i in range(N):
        entry = b[i]
        diagonal = A[i, i]
        if ti.abs(diagonal) < 1e-10:
            print("Diagonal element is too small")

        for j in range(i):
            entry -= A[i, j] * x[j]
        for j in range(i + 1, N):
            entry -= A[i, j] * xOld[j]
        x[i] = entry / diagonal


def solve_gauss_seidel_ti(A, b, x0, max_iterations=100, tolerance=1e-6):
    x = x0.copy()
    for iter in range(max_iterations):
        xOld = x.copy()
        gauss_seidel_kernel(A, b, x, xOld)

        # 计算残差并检查收敛
        r = A @ x - b
        r_norm = np.linalg.norm(r)
        print(f"GS iter {iter}, r={r_norm:.2e}")
        if r_norm < tolerance:
            print(f"Converged after {iter + 1} iterations. Final residual: {r_norm:.2e}")
            return x, r

    print("Did not converge within the maximum number of iterations.")
    print(f"Final residual: {r_norm:.2e}")
    return x, r


def solve_gauss_seidel_sparse(A, b, x0, max_iterations=100, tolerance=1e-6, r_norm_list=[]):
    # gauss seidel is just omega = 1 in SOR
    n = A.shape[0]
    x = x0.copy()

    for iter in range(max_iterations):
        x_new = x.copy()

        for i in range(n):
            Ax_new = A[i, :i].dot(x_new[:i])
            Ax_old = A[i, i + 1 :].dot(x[i + 1 :])
            x_new[i] = (1.0 / A[i, i]) * (b[i] - Ax_new - Ax_old)

        x = x_new.copy()

        # 计算残差并检查收敛
        r = A @ x - b
        r_norm = np.linalg.norm(r)
        r_norm_list.append(r_norm)
        print(f"GS iter {iter}, r={r_norm:.2e}")
        if r_norm < tolerance:
            print(f"Converged after {iter + 1} iterations. Final residual: {r_norm:.2e}")
            return x, r

    print("Did not converge within the maximum number of iterations.")
    print(f"Final residual: {r_norm:.2e}")
    return x, r


def plot_r_norm_list(data, ax, title):
    x = np.arange(len(data))
    ax.plot(x, data, "-o")
    ax.set_title(title)
    ax.set_yscale("log")
    ax.set_xlabel("iteration")
    ax.set_ylabel("residual")


if __name__ == "__main__":
    test_amg_vs_jacobi()
